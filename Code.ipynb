{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Label Efficient Learning of Transferable Representations Across Domains and Tasks](6621-label-efficient-learning-of-transferable-representations-acrosss-domains-and-tasks.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLDR;\n",
    "\n",
    "\n",
    "## NOTES\n",
    "\n",
    "### TODO\n",
    "1. Process the MNIST and SVNH Dataset as required:  Done\n",
    "  * MNIST : 5-9 Classes - Done (Target Domain)\n",
    "  * SVNH : 0-4 Classes - Done (Source Domain)\n",
    "2. Creation of Architecture as shown in the image : ![architecture](resources/Architecture.png) - Keras Issue - Need Debugging\n",
    "3. Train the Model with Optimiser Specified Separately and load the weights in the main experiment.\n",
    "4. Initialise Weights only for Source CNN Model with the pretrained weights.\n",
    "5. Proceed with Trainning the Entire Architecture.\n",
    "\n",
    "\n",
    "\n",
    "# Six different methods : \n",
    "* (i) Target only: the model is trained on D2 from scratch; \n",
    "* (ii) Fine-tune: the model is pretrained on D1 and fine-tuned on D2;\n",
    "* (iii) Matching networks [70]: we first pretrain the model on D3, then use D2 as the support set in the matching networks; \n",
    "* (iv) Fine-tuned matching networks: same as baseline iii, except that for each k the model is fine-tuned on D2 with 5-way (k 􀀀 1)-shot learning: k 􀀀 1 examples in each class are randomly selected as the support set, and the last example in each class is used as the query set; \n",
    "* (v) Fine-tune + adversarial: in addition to baseline ii, the model is also trained on D1 and D3 with a domain adversarial loss; \n",
    "* (vi.) Full model: fine-tune the model with the proposed multi-layer domain adversarial loss.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Module : # TO DO:\n",
    "Softmax with Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.tensor import Tensor as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.datasets import MNIST,SVHN\n",
    "\n",
    "print(torch.__version__) # torch Version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparams:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Hyperparams:\n",
    "bs=64\n",
    "shuffle=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f111a9801e2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0msvnh_ds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCustomSVNH\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mmnist_ds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCustomMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mmnist_dl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f111a9801e2e>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, split, transform, target_transform, download, max_label)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mCustomSVNH\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSVHN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\infi\\lib\\site-packages\\torchvision\\datasets\\svhn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, split, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             raise RuntimeError('Dataset not found or corrupted.' +\n\u001b[1;32m---> 53\u001b[1;33m                                ' You can use download=True to download it')\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m# import here rather than at top of file because this is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "# To Do:\n",
    "# Filter the Dataset Based on Range of Labels Given\n",
    "class CustomMNIST(MNIST):\n",
    "    \"\"\"\n",
    "    Additional Args:\n",
    "        min_label : Minimun Value of Label That is to be allowed. \n",
    "        # Todo : Need to support list of values instead of min value\n",
    "    \"\"\"\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False,min_label=5):\n",
    "        super().__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
    "        self.min_label=min_label\n",
    "        if train:\n",
    "            self.train_data=self.train_data[self.train_labels>=self.min_label]\n",
    "            self.train_labels=self.train_labels[self.train_labels>=self.min_label]\n",
    "\n",
    "class CustomSVNH(SVHN):\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None, download=False,max_label=5):\n",
    "        super().__init__(root, split=split, transform=transform, target_transform=target_transform, download=download)\n",
    "        self.max_label=max_label\n",
    "        if split=='train':\n",
    "            self.data=self.data[self.labels<=self.max_label]\n",
    "            self.labels=self.labels[self.labels<=self.max_label]\n",
    "\n",
    "svnh_ds=CustomSVNH('.',max_label=4)\n",
    "mnist_ds=CustomMNIST('.',min_label=5)\n",
    "mnist_dl=DataLoader(mnist_ds,batch_size=bs,shuffle=shuffle)\n",
    "svnh_dl=DataLoader(svnh_ds,batch_size=bs,shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checked And working as expected\n",
    "# mnist=FilteredMnist('.',download=True,label_min=5)\n",
    "# print((mnist.train_labels))\n",
    "# svnh=CustomSVNH(root='.',split='train',download=True,max_label=4)\n",
    "# mnist=CustomMNIST(root='.',train=True,download=True,min_label=5)\n",
    "# print(svnh.data.shape,svnh.labels.shape)\n",
    "# #print(np.unique(svnh.labels))\n",
    "# #print(np.unique(mnist.train_labels))\n",
    "# print(mnist.train_data.shape,mnist.train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MNIST : 5-9 Classes -  (Target Domain)\n",
    "#  SVNH : 0-4 Classes -  (Source Domain)\n",
    "\n",
    "svnh_ds=CustomSVNH('.',max_label=4)\n",
    "mnist_ds=CustomMNIST('.',min_label=5)\n",
    "bs=64\n",
    "mnist_dl=DataLoader(mnist_ds,batch_size=bs,shuffle=True)\n",
    "svnh_dl=DataLoader(svnh_ds,batch_size=bs,shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# def softmax_temperature(logits, tau=1.0, hard=False, eps=1e-10):\n",
    "#     # Use inbuilt Torch Functional Version of Smoothened Softmax\n",
    "#     return F.gumbel_softmax(logits=logits, tau=tau, hard=hard, eps=eps)  \n",
    "\n",
    "\n",
    "def softmax_temperature(out, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    out = np.log(out) / temperature\n",
    "    out = np.exp(out) / np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, out, 1))\n",
    "\n",
    "def distillation(y, teacher_scores, labels, T, alpha):\n",
    "    p = F.log_softmax(y/T, dim=1)\n",
    "    q = F.softmax(teacher_scores/T, dim=1)\n",
    "    l_kl = F.kl_div(p, q, size_average=False) * (T**2) / y.shape[0]\n",
    "    l_ce = F.cross_entropy(y, labels)\n",
    "    return l_kl * alpha + l_ce * (1. - alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# To Do Finalise on the Architecture\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    \n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "        # self.conv1=nn.Conv2d()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (infi)",
   "language": "python",
   "name": "infi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
